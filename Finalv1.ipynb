{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/BerkeleySeal.png\" width=\"150\"/> <img src=\"resources/berkeleyischool-logo-blue.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Final project for course W261 - Machine Learning in Scale - Spring 2019</center>\n",
    "\n",
    "### __Team 13:__ Clayton Leach, James Kajdasz,  Marcelo Queiroz, Peter Trenkwalder, Vishal Agarwal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "Online advertising is a growing industry whose revenues reached 83 billion dollars in 2017, a 14% growth from 2016. Final users, however, view online advertisement as an unwanted distraction, with little to no benefits to their browsing experience, which led to widespread use of ad-blockers, stressing the model of \"free\" content generation on the internet. \n",
    "\n",
    "For that reason, improving advertisement techniques is an active research field, trying to improve both user and seller experience and profits. From a user perspective, it would be better to see only adds that are related to his/her interests, avoiding distractions that are \"useless\". From the seller perspective, posting the adds in better places and for interested users means more sales with less advertisement, in other words, more efficiency. This kind of targeting is what we call Click-Through-Rate analisys and, for a better understanding, we add two examples bellow:\n",
    "\n",
    "__Example 1:__\n",
    "In display advertising, content providers are often charged per thousand impressions (CPM), as opposed to sponsored search and product listing ads which generally charge on a per click basis (Chapelle et al., 2014).  As such, there is significant uncertainty from the viewpoint of a purchaser as to whether their marketing spend will produce results.  For instance, they might pay 10 dollars per 1000 impressions, (a cent per impression), but receive a single click, for an effective cost per click (CPC) of 10 dollars.  For most advertisers this would be unacceptable and they would cancel their contract.  As such, it is in the interest of an entity serving ads to find a balance between serving ads which pay the most, and serving ads to the correct individuals such that returns for their clients are acceptable.  The best way to manage these sometimes competing priorities is to be able to effectively predict the likelihood of a click conditional on a specific ad being displayed.  In this fashion a company is able to approach serving ads as an optimization problem, where the goal is to maximize revenue constrained by the fact that marketers will churn either due to poor performance, or lack of activity.  A company could also elect to forego the revenue maximization route, and instead target growth by appealing to users; in this framework a company serving ads would look to serve ads which are most relevant, which could be proxied as those most likely to be clicked.  This sentiment is captured by researchers at Facebook: “Machine learning plays a central role in computing the expected utility of a candidate ad to a user, and in this way increases the efficiency of the marketplace” (He et al., 2014).  And the utility of this task becomes even more important under a cost per click scheme, such as what Facebook uses.  In this framework the value of showing an ad can be predicted directly, as CPC * P(click|ad), where P(click|ad) is predicted using machine learning.\n",
    "\n",
    "\n",
    "__Example 2:__\n",
    "While CTR analysis is extremely useful from the perspective of a company serving ads, it is also useful from the perspective of a company purchasing ads.  For instance, if a company can understand the behavioral characteristics of the individual most likely to click on their ad, they can gain valuable insight into their customers.  And there is value on the opposite side of the spectrum as well: understanding the characteristics of a user who is least likely to click on an ad can help inform decisions made with regards to other marketing channels (E.g. if we know users who demonstrate quality x don’t click, then we should avoid T.V/Radio/Magazines/etc. who appeal to a market with a high propensity for quality x).\n",
    "\n",
    "\n",
    "This work uses data from [Criteo](https://www.criteo.com/) on online advertisement and was first posted in a Kaggle competition in 2015. The goal is to use the information about adds presented during 7 days to web users and labeled as a success (add was clicked) or a fail (add was not clicked). We will train a model based on a Binary Logistic Regression algorithm to predict an add success probability that could be embedded in a platform and used to show add that are more interesting to the final user.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Question Formulation\n",
    "\n",
    "The dataset we are provided consists of 13 integer valued features, 26 categorical features, and a boolean indicator for whether the ad was clicked. Also important to note that we are not provided the meaning of any of the features, so no semantic knowledge or domain expertise can be used. Our analysis of the this data seeks to answer the following:\n",
    "\n",
    "* Based on the given data, how accurate can we predict click-through-rate (CTR)?\n",
    "\n",
    "* What are the distributions for our target variable and integer valued features?  Are there any anomalous values, or significant missing values?  \n",
    "\n",
    "* Are there variables which are strongly correlated with the target variable, or with each other?\n",
    "\n",
    "* What are the most influential features/variables in predicting whether a click will happen?\n",
    "\n",
    "* Are there approaches we can take to manage the dimension of our data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithm Explanation\n",
    "\n",
    "### 2.1 Overview of Logistic Regression\n",
    "Our group will apply a logistic regression model to the problem. We chose logistic regression because it is a very common and widely used algorithm, and the results of a logistic regression are relatively easy to communicate and interpret. Logistic regression is a classification algorithm that uses predictor variables (continuous or categorical) to classify an outcome variable in to one of two categories. The categories are arbitrarily given labels of '0' or '1', but can apply to any decision with two possible outcomes: cancerous/non-cancerous, spam/not-spam, fraudulent/legitimate... Logistic regression can be applied to a problem with more than two categories as well by creating a separate equation for each category: A/not A, B/not B, C/not C... The outcome variable is a probability (ranging 0 to 1) of group membership. The classification with the highest probability is the predicted category label.\n",
    "\n",
    "### 2.2 Logistic Regression Equation\n",
    "Logistic regression aggregates the predictor variables similar to what is done in a standard linear regression. Each input $X_j$ is multiplied times a weight $\\beta_j$ and each input/weight product $X_j \\beta_j$ is added together. Or, in summarised form:  \n",
    "\n",
    "$$\\displaystyle f(X)= \\beta_0 + \\Sigma_{j=1}^p X_j \\beta_j$$\n",
    "\n",
    "In matrix algebra form, this can be written as $\\displaystyle f(X)= \\theta^TX$, where $\\theta$ is a vector of weights (including $\\beta_0$), and $X$ is a vector of inputs (with an input of 0 for $\\beta_0$). The modification that logistic regression makes is to then embed the output of $\\theta^TX$ in a new funtion $g(z)$ where $\\displaystyle g(z)=\\frac{1}{1+e^{-z}}$. To put all this together, $h_\\theta (x) = g(\\theta^Tx)$ where $g(z)=\\frac{1}{1+e^{-z}}$. The function $g(z)$ is the sigmoid function, and it has the beneficial property of scaling all outputs between values of 0 and 1. We can write the equations above even more succinctly by substituting in $\\theta^TX$ for $z$. Our final simplified equation is then: \n",
    "\n",
    "$$\\displaystyle h_\\theta (x) = \\frac{1}{1+e^{-\\theta^TX}}$$ \n",
    "\n",
    "We treat the value $h_\\theta(x)$ as our estimate of probability that x is a member of category $y=1$. The probability that $y=0$ will then be $1 - h_\\theta(x)$. Both probabilities will add to one. Recall that $h_\\theta(x)$ ranges from 0 to 1 thanks to our application of the sigmoid function.       \n",
    "\n",
    "### 2.3 Cost Function\n",
    "The weights of a logistic regression equation can vary, and so there must be a way to compare one hypothetical set of weights to another to judge if one model fits the data better than another. This is done by calculating the overall error of the model when attempting to predict $y$ to summarize model fit. The function that computes the error of the model is known as the cost or loss function. The goal of a model is to fit the data in such a way as to minimize the cost function. For a given set of weights $\\theta$ attempting to preditct a label $y_i$, the cost function of $h_\\theta(X)$ can be quantified by simply squaring the errors where cost is $Cost \\displaystyle (h_\\theta (x_i), y_i) = \\frac{1}{2} (h_\\theta(X_i)-y_i)^2$. This is the standard cost function (known as squared loss) for linear regression. For logistic regression however, the squared loss function is not convex and has many local minima. Another alternative for the cost function must therefore be used. Alternatives include hinge loss, and logistic loss. The logistic loss function is used by Apache Spark, according to the [documentation](https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression). For logistic loss (log loss for short), we will take the negative log of the logistic regression output when the actual value of $y$ is 1. When the actual value of $y$ is 0, we will take the negative log of 1 minus the logistic regression output. Written mathematically: $\\displaystyle Cost(h_\\theta(x),y)= \\begin{cases} -log(h_\\theta(X)) & y=1\\\\-log(1-h_\\theta(X)) & y=0\\end{cases}$ The log loss function has some nice properties. When the logistic regression correctly predicts that $\\hat{y}=1$ with a probability of 1, then $-log(1)=0$ and the loss function is zero, reflecting a perfect prediction. Similarlry, if we (correctly) predict that $\\hat{y}:0$ with a probability of 1, the cost function will be $-log(1-1)=0$. If however we make an incorrect prediction that $P(\\hat{y}:0)=.999$, (and the corresponding probability $P(\\hat{y}:1)=.001$) when in actuality $y=1$, then the log loss function will be $-log(.001)\\approx3$, reflecting a higher amount of error. Note that we can't take the log of 0, so instead we use values of .999 and .001. As the correct prediction approaches a probability of 0, the log loss function will approach infinity. the prediction is $y=0$     \n",
    "\n",
    "#### 2.4 Finding the Correct Model\n",
    "One could select logistic regression weights at random, and see if the new model is an improvement over the last model by evaluating the loss function, and continue iterating. This is inefficient however and there is no gaurantee we'd ever stumble across the best model by chance. It's better to have an algorithm that systematively moves us to better and better models. There are many different algorithms to choose from. When dealing with data at scale, consideration of algorithm will also need to take into account the number of iterations required and how fast an algorithm will converge. For working with data at scale, the Apache Spark [documentation](https://spark.apache.org/docs/latest/mllib-optimization.html#choosing-an-optimization-method) recommends the Limited-memory Broyden-Fletcher-Golfarb-Shanno algorithm (L-BFGS for short)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EDA and Discussion of Challenges\n",
    "\n",
    "### 3.1 Infrastructure:\n",
    "\n",
    "Handling a 45 million rows dataset with 10.38 GB is not easy when using a single machine, even using Spark Dataframes. For that reason, our approach was to use a cluster in Google Cloud with a storage bucket attached to it. However those operations can incur in high cost, so we decided to export a 5% random sample from our dataset to a local machine to perform our exploratory data analysis and implement our a test model. After that, we can run the same notebook in the cluster again using a PySpark kernel and taking advantage of distributed worker nodes. In this notebook, you will find cells made to run on a cluster, with the whole dataset, unless explicitly stated otherwise in comments at the first line of the cell.\n",
    "\n",
    "To spin up the cluster we [installed the Google Cloud Platform command line interface](https://cloud.google.com/sdk/docs/quickstart-macos) in our local machine and used the `GCP-create-cluster.sh` shell script set with Spark and other dependencies needed for this project on 6 worker and 1 master node, all consisting of Google's `n1-standard-8` machines (up to 100 GB of disk and 30 GB of memory, with 8 virtual CPUs). After that we can run the other two scripts: `GCP-create-proxy.sh` and `GCP-use-proxy.sh` in this order and in different comand line windows. This will open a Chrome browser connected to the cluster via proxy and where we can run notebooks directly. The scrips can be found in this repository in the `shell-scripts` folder. With that set, we can start working on our cluster to handle the dataset:\n",
    "\n",
    "### 3.2 Creating Toy Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from os import path\n",
    "\n",
    "# import Spark dependencies\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql import SQLContext, functions\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppressing or enabling warnings. Usefull for debugging.\n",
    "# warnings.filterwarnings('ignore')\n",
    "# warnings.resetwarnings()\n",
    "\n",
    "# setting notebook paths:\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Spark Session:\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"w261-FinalProject\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the dataset file for it's form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloadin and uncompressing the dataset, we see that we have three text files: `train.txt`, `test.txt`, and `readme.txt`. To start dealing with our dataset, we want to define a schema for it. Reading the file `readme.txt`, we have:\n",
    "\n",
    "```\n",
    "====================================================\n",
    "\n",
    "Format:\n",
    "\n",
    "The columns are tab separeted with the following schema:\n",
    "<label> <integer feature 1> ... <integer feature 13> <categorical feature 1> ... <categorical feature 26>\n",
    "\n",
    "When a value is missing, the field is just empty.\n",
    "There is no label field in the test set.\n",
    "\n",
    "====================================================\n",
    "```\n",
    "\n",
    "This way, we see that using the test set is not feasible, once we don't know the correct answers to our predictions. Our solution is to withhold 30% of our train dataset and use it to test our model. Based on the schema described, we will read our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13 features taking integer values and 26 taking categorical values (we can't assume ordinal) - making total of 39 features + 1 outcome\n",
    "outcomeField = [StructField(\"label\", IntegerType(), True)]\n",
    "quantFields = [StructField(\"intFeature\"+str(i+1), IntegerType(), True) for i in np.arange(13)]\n",
    "qualFields = [StructField(\"catFeature\"+str(i+1), StringType(), True) for i in np.arange(26)]\n",
    "schema = StructType(outcomeField + quantFields + qualFields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the txt file\n",
    "file = \"gs://w261_final-project_team13/train.txt\"\n",
    "df_full = spark.read.schema(schema) \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"ignoreLeadingWhiteSpace\",True) \\\n",
    "    .option(\"ignoreTrailingWhiteSpace\",True) \\\n",
    "    .csv(file, nullValue = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting:\n",
    "print('Total dataset count:', df_full.count(), 'observations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the splits and \n",
    "trainDF, testDF = df_full.randomSplit([0.7,0.3], 2019)\n",
    "print('Train dataset count:', trainDF.count(), 'observations.')\n",
    "print('Test dataset count:', testDF.count(), 'observations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, we will take 5% of the dataset as sample, which will be roughly $45,840,617 \\cdot 0.05  \\cdot 0.7 = 1,604,421.595$ observations and $10.38E3 \\cdot 0.05 \\cdot 0.7 = 360$ MB in a txt form, which can be reasonobly handled in a single node machine and still relevant. Using a more efficient file form would make things even better, so we decided to use parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the toy set for single-node handling:\n",
    "toyTrainDF = trainDF.sample(False, 0.05)\n",
    "toyTestDF = testDF.sample(False, 0.05)\n",
    "\n",
    "# writing to our CGP storage bucket as a parquet file:\n",
    "toyTrainDF.write.parquet(\"gs://w261_final-project_team13/toy_train.txt\")\n",
    "toyTestDF.write.parquet(\"gs://w261_final-project_team13/toy_train.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are able to download this parquet file in our local machines and run in our own notebooks for performing EDA and even a the implementation of our algorithms in a small data set. To download it we can use GCP's user interface or the command line interface. The following code will copy the files to a local folder called `data`:\n",
    "```\n",
    "gsutil -m cp gs://w261_final-project_team13/toy_test.txt/* .data/toy_test.txt/\n",
    "gsutil -m cp gs://w261_final-project_team13/toy_train.txt/* .data/toy_train.txt/\n",
    "```\n",
    "Additionally we may want to copy the notebook as well:\n",
    "```\n",
    "gsutil cp gs://w261_final-project_team13/notebooks/* ./QuestionFormulation.ipynb\n",
    "```\n",
    "\n",
    "Now we can read that data and start working on it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Running on local single-node machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAN BE RUN IN LOCAL NODE\n",
    "\n",
    "# read the parquet files and print the first observations of each:\n",
    "toyTrainDF = spark.read.parquet(\"./data/toy_train.txt\")\n",
    "toyTestDF = spark.read.parquet(\"./data/toy_test.txt\")\n",
    "\n",
    "# print main numbers from our DF:\n",
    "print('Train dataset count:', toyTrainDF.count(), 'observations.')\n",
    "print('Test dataset count:', toyTestDF.count(), 'observations.')\n",
    "toyTrainDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid confusion and using the test dataset in our model, we will now use only the `toyTrainDF` for EDA. Starting with exploring the outcome feature (`label`), we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAN BE RUN IN LOCAL NODE\n",
    "\n",
    "toyTrainDF.describe(\"label\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "1. No missing labels\n",
    "2. About 25% of the data has label of 1, rest is 0\n",
    "\n",
    "#### 3.3.1 Exploring the integer features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAN BE RUN IN LOCAL NODE\n",
    "\n",
    "intField = [\"intFeature\"+str(i+1) for i in np.arange(13)]\n",
    "df_eda.describe(intField[:6]).show()\n",
    "df_eda.describe(intField[6:]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "1. `intFeature12` has a very large number of nulls (~80%). We must consider removing it from the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAN BE RUN IN LOCAL NODE\n",
    "\n",
    "# setting a list with features to drop\n",
    "features_to_drop = ['intFeature12']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations (continued):\n",
    "2. The spread in values is very large for almost all the features. We can draw a histogram to understand the distribution of the feature values.\n",
    "3. Feature intFeature4 seems to be very concentrated around the mean. We can draw a histogram to better understand the distribution.\n",
    "4. intFeature2 has at least one negative value. Let's take a closer look at those values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAN BE RUN IN LOCAL NODE\n",
    "\n",
    "toyTrainDF.select('I2').filter(toyTrainDF['I2'] < 0).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the value count is less than 1% of the dataset, containing `-1`, `-2`, and `-3` values. As this dataset were most probably collected without human interference, this is not likely to be a typo. For this analisys, we will consider that negative values are error codes, and as we are not able to infer if those errors have semantic meaning or not, we can simply discard those values replacing them by `null` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAN BE RUN IN LOCAL NODE\n",
    "\n",
    "# use a simple user defined function to replace negative values by null:\n",
    "replace_negatives = function.udf(lambda x: None if x < 0 else x, IntegerType())\n",
    "toyTrainDF = toyTrainDF.withColumn('intFeature2', replace_negatives(df.intFeature2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, addressing topics 2 and 3, we can take a look on the histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAN BE RUN IN LOCAL NODE\n",
    "\n",
    "df_eda_pandas.hist(figsize=(14,14), bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "1. Most features are highly left-skewed. We can minimize that problem using log transformations.\n",
    "2. The features should be normalized so they all appear on the same scale.\n",
    "\n",
    "Additionally, we can check for correlations between variables, once logistic regression models assume no multicolinearity and highly correlated variables can introduce bias in our predictions. To do both feature transformations and correlations calculation at the same time, we can use a scatterplot matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'toyTrainDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f217a182f8cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# re-sampling the DF for better visualization:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mvizDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoyTrainDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2019\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# log transform and normalize the DF, keeping the label column and using (VALUE - MEAN/STDDEV)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'toyTrainDF' is not defined"
     ]
    }
   ],
   "source": [
    "# CAN BE RUN IN LOCAL NODE\n",
    "\n",
    "def corrdot(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Helper function to plot correlation indexes in the upper side of the scatterplot matrix.\n",
    "    Reference: https://github.com/mwaskom/seaborn/issues/1444\n",
    "    \"\"\"\n",
    "    corr_r = args[0].corr(args[1], 'spearman')\n",
    "    # Axes\n",
    "    ax = plt.gca()\n",
    "    ax.axis('off')\n",
    "    x_min, x_max = ax.get_xlim()\n",
    "    x_centroid = x_min + (x_max - x_min) / 2\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    y_true = y_max - (y_max - y_min) / 4\n",
    "    y_false = y_min + (y_max - y_min) / 4\n",
    "    # Plot args\n",
    "    if kwargs['label'] == True:\n",
    "        marker_size = abs(corr_r) * 5000\n",
    "        ax.scatter(x_centroid, y_true, marker='o', s=marker_size, alpha=0.6, c='red')\n",
    "        corr_text = str(round(corr_r, 2)).replace('-0', '-').lstrip('0')\n",
    "        ax.annotate(corr_text, [x_centroid, y_true,], ha='center', va='center', fontsize=20)\n",
    "    else:\n",
    "        marker_size = abs(corr_r) * 5000\n",
    "        ax.scatter(x_centroid, y_false, marker='o', s=marker_size, alpha=0.6, c='green')\n",
    "        corr_text = str(round(corr_r, 2)).replace('-0', '-').lstrip('0')\n",
    "        ax.annotate(corr_text, [x_centroid, y_false,], ha='center', va='center', fontsize=20)\n",
    "\n",
    "# re-sampling the DF for better visualization:\n",
    "vizDF = toyTrainDF.sample(False, 0.001, 2019).toPandas().iloc[:,0:14]\n",
    "\n",
    "# define helper function to log-tranform:\n",
    "def log_transform(x):\n",
    "    '''shifts x up by one to account for presence of 0 values'\n",
    "    and assumes negative values should be NaN'''\n",
    "    return np.log(x + 1)\n",
    "\n",
    "# log transform and normalize the DF, keeping the label column and using (VALUE - MEAN/STDDEV)\n",
    "vizDF_norm = pd.DataFrame(vizDF[quantFields].values, columns=quantFields, index=vizDF.index)\n",
    "vizDF_norm.apply(log_transform)\n",
    "vizDF_norm = (vizDF_norm - vizDF_norm.mean())/vizDF_norm.std()\n",
    "vizDF[quantFields] = vizDF_norm\n",
    "\n",
    "# ploting the results splited by label:\n",
    "sns.set_context('notebook', font_scale=1.3)\n",
    "g = sns.PairGrid(vizDF, vars=quantFields, hue='label', palette={True:'green', False:'red'})\n",
    "g.map_lower(sns.scatterplot, alpha=0.3)\n",
    "g.map_diag(sns.distplot, hist=False)\n",
    "g.map_upper(corrdot)\n",
    "g.add_legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "1. We consider highly correlated features where Pearson's index is higher than 0.8. The following features shows strong correlations:\n",
    "    * `intFeature1` and `intFeature10`\n",
    "    * `intFeature4` and `intFeature13`\n",
    "    * `intFeature5` and `intFeature10`\n",
    "    * `intFeature7` and `intFeature11`\n",
    "2. We can reduce multicolinearity dropping features 10, 11 and 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAN BE RUN IN LOCAL NODE\n",
    "\n",
    "# adding features to drop list:\n",
    "features_to_drop.append(['intFeature10', 'intFeature11', 'intFeature13'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Exploring the categorical features:\n",
    "\n",
    "As instated before, we don't have any clue about the semantic value of the categorical features, so our analysis will be based solely on data completion and distribution. First let's take a look if missing data means less probability of a click:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAN BE RUN IN LOCAL NODE\n",
    "\n",
    "# checking the average Null count in each row:\n",
    "toyTrainPandasDF = toyTrainDF.toPandas()\n",
    "countNullDF = toyTrainPandasDF.isnull().sum(axis=1).rename('CountNaN')\n",
    "countNullDF.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAN BE RUN IN LOCAL NODE\n",
    "\n",
    "# concat dataframes\n",
    "countNullDF = pd.concat([toyTrainPandasDF['label'], countNullDF], axis=1)\n",
    "countNullDF.groupby('label').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that some rows have only null values, while the average missing count is around 5. Also we note a difference in null count for clicked and non-clicked add, however the difference can be considered neglegible.\n",
    "\n",
    "Now, let's consider the distribution of the categorical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAN BE RUN IN LOCAL NODE\n",
    "\n",
    "uniqueCounts = {}\n",
    "for field in catField:\n",
    "    uniqueCounts[field] = df_eda.select(field).distinct().count()\n",
    "plt.bar(uniqueCounts.keys(), uniqueCounts.values(), color='g')\n",
    "plt.title(\"Unique values in each categorical features\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "Some categorical variables (such as catFeature3) have a large number of unique values. They should be binned using some techniques. \n",
    "<br>(a) They can be binned based on frequency.\n",
    "<br>(b) They can be binned based on average value of the outcome variable.\n",
    "\n",
    "For a better understand of each feature, we can take a look on its main values and value count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAN BE RUN IN LOCAL NODE\n",
    "\n",
    "def descCatEDA(dataframe, column, totalCount=0, nanThreshold=0.5):\n",
    "    \"\"\"\n",
    "    Function that prints an analysis of column from the given dataframe. Retuns None.\n",
    "    Args:\n",
    "        dataframe    - input dataframe\n",
    "        column       - string with a dataframe column.\n",
    "        totalCount   - optional. Number of rows in the dataframe (if defined avoid recalculation).\n",
    "        nanThreshold - optional. Percentage allowed of NaN values in the column.\n",
    "    Returns:\n",
    "        Column       - name of the column if the column have more NaN if it represents more \n",
    "                       than nanThreshold ratio.\n",
    "    Output:\n",
    "        NaN Count    - number for NaN values in % of the row count of the dataframe.\n",
    "        Most Freq    - number of values for the 5 most frequent values (discarding NaN).\n",
    "        \n",
    "    \"\"\"\n",
    "    if totalCount == 0:\n",
    "        totalCount = dataframe.count()\n",
    "    \n",
    "    pandCol = dataframe.select(column).toPandas()[column]\n",
    "    freqNumbers = dict(pandCol.value_counts(normalize=True).head(5))\n",
    "    \n",
    "    nanCount = dataframe.filter(dataframe[column].isNull()).count()\n",
    "    \n",
    "    validCount = totalCount - nanCount\n",
    "    \n",
    "    print('+'+13*'-'+'+'+22*'-'+'+')\n",
    "    print('| {:^4}'.format(column)+'|{:>22}{:>6.2f}% |'.format('Null Count: ', nanCount/totalCount*100))\n",
    "    print('+'+13*'-'+'+'+22*'-'+'+')\n",
    "    print('| Unique Values: {:>19} |'.format(pandCol.nunique()))\n",
    "    print('+'+13*'-'+'+'+22*'-'+'+')\n",
    "    for item in freqNumbers:\n",
    "        print('|{:>12} |{:>20.2f}% |'.format(item, freqNumbers[item]*100))\n",
    "    print('+'+13*'-'+'+'+22*'-'+'+\\n')\n",
    "    \n",
    "    if nanCount/totalCount*100 > nanTreshold*100:\n",
    "        return column\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAN BE RUN IN LOCAL NODE\n",
    "\n",
    "badFeatures = []\n",
    "totalCount = toyTrainDF.count()\n",
    "nanTreshold = 0.75\n",
    "\n",
    "for item in catField:\n",
    "    badFeatures.append(descCatEDA(df_eda, item, totalCount, nanTreshold))\n",
    "\n",
    "badFeatures = list(filter(None,badFeatures))\n",
    "print('List of catFeatures with more than {:4.2f}% NaN ratio: {}'.format(nanTreshold*100, badFeatures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "1. Some features have a high number of missing data (such as `catFeature22`, missing more than 75% of the values). With this result we can set a threshold to drop features.\n",
    "2. Similarly to what happened with the integer features, some are highly skewed, with up to 90% of the valid values in one category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAN BE RUN IN LOCAL NODE\n",
    "\n",
    "# adding features to drop list:\n",
    "features_to_drop.append(['catFeature22'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, in order to apply the Logistic Regression model into our data, we want to one-hot-encode our levels. However, depending on the number of unique elements in our features, this approach can generate a high number of features, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAN BE RUN IN LOCAL NODE\n",
    "\n",
    "uniqueCounts = {}\n",
    "for field in strColumns:\n",
    "    uniqueCounts[field] = toyTrainDF.select(field).distinct().count()\n",
    "uniqueCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAN BE RUN IN LOCAL NODE\n",
    "\n",
    "avg_levels = sum(uniqueCounts.values())/len(uniqueCounts.values())\n",
    "avg_levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check numbers!\n",
    "In this case, we can have $26 \\cdot 12161 = 316,186$ in a higly sparse dataset. Based on our toy datset we will have a features/rows ratio of about $\\frac15$, where we already start having problems with the curse of dimensionality. The way we found to overcome this problem was to work with bins for rare leves. Hopefully, with the skeweness of our data, this technique will drastically reduce the model dimesionality.\n",
    "\n",
    "Finally, because of the lack of semantic meaning of categorical data, the absence of data may also mean something for our model, so we want to add missing values on categorical data to its own bin, creating bins of `missing_value` for each categorical feature.\n",
    "\n",
    "After all the previous consideration about our data, we are ready to start transforming our dataset and implementing the Binary Logistic Regression to predict click-through-rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Algorithm Implementation\n",
    "\n",
    "### 4.1 Data preparation:\n",
    "From the previous sections we derived the following tranformations for our dataset:\n",
    "1. Drop `intFeature12`.\n",
    "2. Set negative numbers in `intFeature2` to `null`.\n",
    "3. Apply a log-tranformation and normalized all numeric features.\n",
    "4. Drop features `intFeature10`, `intFeature11`, and `intFeature13`.\n",
    "5. Drop feature `catFeature22`.\n",
    "6. Create bins `rare-levels` for levels with less than 30 occurencies on categorical features.\n",
    "7. Create bins `missing_values` for categorical data.\n",
    "\n",
    "Additionally we will adress more common data tranformation, like deleting duplicated rows and rows missing more than 75% of the features. Let's starting defining a function to transform our dataframe (we will use now for the train data, and later for the test data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_transform(df, features_to_drop, feature_threshold):\n",
    "    '''\n",
    "    Function to apply all tranformations described in EDA section:\n",
    "        1. Drop features_to_drop.\n",
    "        2. Set negative numbers in `intFeature2` to `null`.\n",
    "        3. Apply a log-tranformation and normalized all numeric features.\n",
    "        4. Create bins `rare-levels` for levels with less than 30 occurencies on categorical features.\n",
    "        5. Create bins `missing_values` for categorical data.\n",
    "    Args:\n",
    "        df                : pyspark dataframe containing all features \n",
    "        features_to_drop  : features identified to be dropped\n",
    "        feature_threshold : features with less observations than this will be\n",
    "                            converted to 'feature_name-rare'\n",
    "    Returns:\n",
    "        transformed_DF    : pyspark dataframe with the same form, but with transformed features.\n",
    "    '''\n",
    "    \n",
    "    # Cleaning duplicate and missing more than 75% of the features:\n",
    "    dedupedDF = df.dropDuplicates()\n",
    "    print('Droped {:f}% duplicated rows: {}'.format(toyTrainDF.count() - dedupedDF.count()))\n",
    "    cleanDF = dedupedDF.dropna(thresh=30)\n",
    "    print('Droped {:f}% incomplete rows: {}'.format(dedupedDF.count() - cleanDF.count()))\n",
    "\n",
    "    # Drop bad features (see EDA section):\n",
    "    cleanDF.drop(features_to_drop)\n",
    "\n",
    "    # Setting intFeature2 values to null (see EDA section for udf definition)\n",
    "    cleanDF = cleanDF.withColumn('intFeature2', replace_negatives(df.intFeature2))\n",
    "    \n",
    "    # log-transform integer features\n",
    "    def log_transform(x):\n",
    "    '''shifts x up by one to account for presence of 0 values'\n",
    "    and assumes negative values should be NaN'''\n",
    "        return functions.log1p(x)\n",
    "\n",
    "    # log transform and normalize the DF, keeping the label column and using (VALUE - MEAN/STDDEV)\n",
    "    cleanDF.apply(log_transform)\n",
    "    cleanDF = (cleanDF - cleanDF.mean())/cleanDF.std()\n",
    "    cleanDF[quantFields] = cleanDF\n",
    "\n",
    "    # Creating new columns with bins for \"rare levels\" and \"missing values\"\n",
    "    for feature in catFeatures:\n",
    "        newCol_name = feature + \"_bin\"\n",
    "        missingBucket = feature + \"_NaN\"\n",
    "        row_elements = []\n",
    "\n",
    "        # count of unique labels per category\n",
    "        uniq = cleanDF[feature].value_counts()\n",
    "\n",
    "        #index of labels with fewer than 30 observations (return the labels with .index)\n",
    "        idx = set(uniq[uniq < feature_threshold].index)\n",
    "        labels = cleanDF[feature].values\n",
    "        for label in labels:\n",
    "            if label in idx:\n",
    "                row_elements.append(feature + \"-rare\")\n",
    "            else:\n",
    "                row_elements.append(label)\n",
    "\n",
    "        # add the new columns to the DF\n",
    "        cleanDF[newCol_name] = row_elements\n",
    "\n",
    "        # fill the missing values with with the bin name\n",
    "        cleanDF.na.fill(missingBucket, subset=newCol_name)\n",
    "\n",
    "    # Drop the unbinned columns\n",
    "    binnedDF = cleanDF.drop(catFeatures)\n",
    "    \n",
    "    return binnedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binnedTrainDF = df_transform(trainDF, features_to_drop, 30)\n",
    "binnedTestDF = df_transform(testDF, features_to_drop, 30)\n",
    "binnedTrainDF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, it is also benefitial to store our tranformed dataset so we can have a checkpoint and restore our work if needed. For that, we will use a parquet file again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binnedTrainDF.write.parquet(\"gs://w261_final-project_team13/train_preprocessed.txt\")\n",
    "binnedTrainDF.write.parquet(\"gs://w261_final-project_team13/test_preprocessed.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Model Training\n",
    "\n",
    "For the model training itself, we decided to go with the PySpark ML library, that is based on PySpark DataFrames, and allows use to quick assmebly and configure practical machine learning pipelines. For reference on how the alogrithm works, see section 2: Algorithm Explanation.\n",
    "\n",
    "In a high level, we will take the pre-processed dataset and set a pipeline, a high level API that allows us to set tranformations and estimations to our dataset (pretty much like tranformatiosn and actions in RDDs). Note that we could add the `df_transform` function to this pipeline and have a pipeline that could ingest raw data and output predictions. We chose to have our data preparation outside of the pipeline was for clarity puporses, however, if this model eventually goes into production, we would add all the data transformations required. For this model, the stages of the pipeline need to be:\n",
    "1. Create indexers to our categorical features\n",
    "2. One-hot-encode using the indexers\n",
    "3. Create a multiple column vector\n",
    "4. Train the model using the train data\n",
    "5. Fit the test data and output the results.\n",
    "\n",
    "# DO WE NEED TO EXPLAIN INDEXERS, VECTORS AND ETC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the new features names:\n",
    "catColumns = [item for item in binnedTrainDF if 'cat' in item]\n",
    "intColumns = [item for item in binnedTrainDF if 'int' in item]\n",
    "target = ['click']\n",
    "\n",
    "# creating the indexers:\n",
    "indexers = [StringIndexer(inputCol=c, outputCol='{0}_indexed'.format(c), handleInvalid='skip') for c in catColumns]\n",
    "label = StrinIndexer(inputCol=target[0], outputCol='label', handleInvalid='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the one-hot-encoder:\n",
    "encoder = OneHotEncoderEstimator(\n",
    "    inputCols=[indexer.getOutputCol() for indexer in indexers],\n",
    "    outputCols=[\"{0}_encoded\".format(indexer.getOutputCol()) for indexer in indexers],\n",
    "    dropLast=False)\n",
    "\n",
    "# creating the vector:\n",
    "assembler = VectorAssembler(\n",
    "    inputCols= encoder.getOutputCols() + numericCols,\n",
    "    outputCol= \"features\")\n",
    "\n",
    "# defining the pipeline:\n",
    "pipeline = Pipeline(stages = indexers + [encoder,assembler,label])\n",
    "\n",
    "# applying the pipeline:\n",
    "procTrainDF = pipeline.fit(binnedTrainDF).transform(binnedTrainDF)\n",
    "procTestDF = pipeline.fit(binnedTrainDF).transform(binnedTrainDF)\n",
    "\n",
    "# Keep relevant columns\n",
    "selectedcols = [\"label\", \"features\"]\n",
    "train = procTrainDF.select(selectedcols)\n",
    "test = procTestDF.select(selectedcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial LogisticRegression model\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data using the transform() method. \n",
    "# LogisticRegression.transform() will only use the 'features' column.\n",
    "predictions = lrModel.transform(test)\n",
    "selected = predictions.select('label', 'prediction', 'probability')\n",
    "\n",
    "\n",
    "# evaluation of logit fit with logLoss\n",
    "# need a user defined function to manipulate columns of array types\n",
    "firstelement=udf(lambda v:float(v[1]),FloatType())\n",
    "\n",
    "# pyspark implementation for logLoss\n",
    "pyspark_mod = selected.withColumn('logloss', -functions.col('label')*functions.log(firstelement(functions.col('probability'))) - (1.-functions.col('label'))*functions.log(1.-firstelement(functions.col('probability'))))\n",
    "\n",
    "pyspark_mod.agg(f.mean('logloss').alias('ll')).collect()[0]['ll']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model can be evaluated using several loss metrics, however we decided to go with the log-loss, which is the metric used on the original competition score board. To define the log-loss function we will use a udf:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Application of Course Concepts\n",
    "\n",
    "### 5.1 Baseline\n",
    "\n",
    "__Zero-Information baseline:__\n",
    "\n",
    "The most basic baseline would be to assume that every ad has equal probability of being clicked, which is equal to the globally observed click through rate.  While in practice this would never be employed, it represents the level which can be achieved without any customization, and therefore is effective in helping to understand the value proposition of any analytical solution, including machine learning.  Since CTR is generally small, we will most likely always end up predicting the majority class, “did not click”.\n",
    "\n",
    "__Empirical-Bayes:__\n",
    "\n",
    "Our Empirical-Bayes baseline is a level above the “zero-information” approach, and represents the best we can do without taking interaction effects into consideration (e.g. how a user’s features influence click probability conditional on the fact that it is this specific ad).  Under this framework we adjust the global user CTR using Empirical-Bayes to predict a user’s CTR, as well as adjust the global ad CTR to predict an ad’s CTR.  These values can then be used to estimate the probability that user will click on that ad.  We point the reader to an intuitive explanation of empirical bayes by David Robinson if they are unfamiliar with this topic (Robinson).\n",
    "\n",
    "From our perspective, our model is “practically useful” if it outperforms our Empirical-Bayes baseline, as that would indicate that taking into account specific features has improved performance vis a vis predicting results by extrapolating historical occurences of the metric in question.\n",
    "\n",
    "\n",
    "### 5.2 Gradient Descent (Convex Optimization): \n",
    "\n",
    "Convex optimization has the beautiful property that we are guaranteed to find the global minimum, as opposed to a local minimum, and therefore can be assured that given a dataset and model structure we achieve the best possible result.  We chose logistic regression as our model, whose loss function and gradient are:\n",
    "\n",
    "$$J(\\theta) = {1 \\over_m} \\cdot (-y^Tlog(h) - (1-y)^Tlog(1-h)) $$\n",
    "$$\\delta J(\\theta) = {1 \\over m} X^T (f(X\\theta)-y)$$\n",
    "$$h = f(X\\theta)$$\n",
    "Where f(x) is sigmoid function\n",
    "\n",
    "Since cross entropy is convex we can be assured our result is a global minimum.\n",
    "\n",
    "__SGD vs. Batch:__\n",
    "    \n",
    "SGD allows us to make significantly more gradient updates per epoch, with the caveat that we might make some changes in the wrong direction, as our gradient is being derived from a single training example.  However, in aggregate these missteps should be irrelevant and the model should converge.  With batch, we use the entire dataset to calculate the gradient, meaning we have to pass over all the data a significant number of times.  Mini-batch strikes a balance between the two, and uses a batch size greater than 1, but much smaller than the size of the dataset.  For our implementation we selected mini-batch, as this allows us to speed up the rate of convergence, while minimizing the risk of a gradient update moving the model in the wrong direction.\n",
    "\n",
    "### 5.3 Data Normalization:\n",
    "\n",
    "While some machine learning methods are robust against variables having different scale, or distribution (e.g. Tree based methods), logistic regression benefits from the normalization of data, and is absolutely necessary if regularization will be used.  There are many ways to normalize data, such as min-max scaling, which maps every feature to be between 0 and 1, as well as Z-score standardization, which transforms the distribution to be ~N(0,1).  In our project we choose to employ Z-score standardization to better handle outliers, which we observed in our EDA.   \n",
    "\n",
    "### 5.4 One hot encoding, model complexity, and dimensionality reduction:\n",
    "\n",
    "Logistic regression is not capable of handling categorical variables, and therefore we had to determine how to transform our 26 categorical variables into numerical values.  One common approach is “one-hot encoding” which involves taking a single categorical variable, and expanding it to n-1 features, where n is the number of unique values in the category; each new feature is a binary indicator for one of the n features.  While at first glance handling 26 categorical variables seems trivial, upon further inspection there is a significant challenge: some categorical variables have tens of thousands of unique values!  If we were to implement “one-hot-encoding” without any additional considerations we would end up with hundreds of thousands of features, many of which would be very sparse.  Including that quantity of features would make the model completely intractable from the perspective of identifying key factors, and would increase the necessary training data to have accurate parameter estimates by several orders of magnitude.  In an effort to avoid these problems we took the following actions: 1) one-hot-encode any category with at least 30 observations in the training data, and map anything else to a special `rare` bucket; additionally, we map all missing values to a special `_NaN` bucket.  This approach ensures we keep the features which are most likely to actually impact a prediction, while avoiding the “curse of dimensionality”.  This approach represents a manual, rule-based approached to dimensionality reduction.  It is worth noting that we could have opted for a raw implementation of one-hot-encoding and then used principal component analysis to reduce dimensionality.\n",
    "\n",
    "### 5.5 Regularization \\& Overfitting:\n",
    "\n",
    "One common problem in machine learning is that of overfitting, where a trained model performs poorly on out of sample tests, as it has “memorized” the training data rather than learned generalizable patterns.  While some models are particularly susceptible to this issue (e.g. LSTMs), it is almost always a concern data scientists should be wary of.  Within neural models, “dropout” is commonly used to avoid overfitting, which is a similar concept to that of excluding certain features randomly within a particular tree in a random forest.  In both linear and logistic regression these options aren’t available to us, so instead we turn to “regularization”.  Regularization involves adding a penalty term to the loss function which increases as parameter weights increase.  This forces the model to avoid large parameter values, unless the added predictive value is large.  There are two main types of regularization, L1 and L2, whose formulas are as follows:\n",
    "\n",
    "$$L_1 = \\lambda \\Sigma_{j=1}^m |\\beta_j|$$\n",
    "$$L_2 = \\lambda \\Sigma_{j=1}^m \\beta_j^2$$\n",
    "\n",
    "L1 is commonly associated with Lasso regression, while L2 is associated with Hinge; for our project we made use of L2.  To evaluate the impact of regularization we trained our model both with and without our regularization term, and compared our results on our withheld test set.  Somewhat surprisingly, the results were nearly identical.  It’s possible that given the size of our feature set, no one variable was being overfit even without regularization.\n",
    "\n",
    "### 5.6 MapReduce Paradigm:\n",
    "\n",
    "The majority of algorithms which are implemented at scale make use of the MapReduce paradigm, and logistic regression is no different.  To calculate our gradients we first need to make predictions for our training examples, each of which will be accomplished with a map function; presumably we will also be broadcasting the current value of Theta, our parameter vector.  We can then calculate the gradient per training example before reducing to arrive at our final update.  By finding ways to complete work in parallel we are able to drastically speed up the process of training/optimizing a model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bibliography\n",
    "\n",
    "[Wikipedia - Online Advertising](https://en.wikipedia.org/wiki/Online_advertising)\n",
    "\n",
    "[Chapelle, Olivier, et al. “Simple and Scalable Response Prediction for Display Advertising.” ACM Transactions on Intelligent Systems and Technology](https://www.dropbox.com/s/s4x7wp8gjsh021d/TISTRespPredAds-Chappelle-CTR-Prediction-2014Paper.pdf?dl+=0)\n",
    "\n",
    "[He, Xinran, et al. “Practical Lessons from Predicting Clicks on Ads at Facebook.” Proceedings of 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining](https://research.fb.com/publications/practical-lessons-from-predicting-clicks-on-ads-at-facebook/)\n",
    "\n",
    "[Robinson, David. “Understanding Empirical Bayes Estimation (Using Baseball Statistics).” Variance Explained](varianceexplained.org/r/empirical_bayes_baseball/)\n",
    "\n",
    "[Nagpal, Anuja. “Logistic Regression from Scratch in Python.” Medium, Medium, 23 Feb. 2018](medium.com/@martinpella/logistic-regression-from-scratch-in-python-124c5636b8ac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
