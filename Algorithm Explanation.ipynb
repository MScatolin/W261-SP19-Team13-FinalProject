{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/BerkeleySeal.png\" width=\"150\"/> <img src=\"resources/berkeleyischool-logo-blue.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Final project for course W261 - Machine Learning in Scale - Spring 2019</center>\n",
    "\n",
    "### __Team 13:__ Clayton Leach, James Kajdasz,  Marcelo Queiroz, Peter Trenkwalder, Vishal Agarwal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Question Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithm Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of Logistic Regression\n",
    "Our group will apply a logistic regression model to the problem. We chose logistic regression because it is a very common and widely used algorithm, and the results of a logistic regression are relatively easy to communicate and interpret. Logistic regression is a classification algorithm that uses predictor variables (continuous or categorical) to classify an outcome variable in to one of two categories. The categories are arbitrarily given labels of '0' or '1', but can apply to any decision with two possible outcomes: cancerous/non-cancerous, spam/not-spam, fraudulent/legitimate... Logistic regression can be applied to a problem with more than two categories as well by creating a separate equation for each category: A/not A, B/not B, C/not C... The outcome variable is a probability (ranging 0 to 1) of group membership. The classification with the highest probability is the predicted category label.\n",
    "\n",
    "#### Logistic Regression Equation\n",
    "Logistic regression aggregates the predictor variables similar to what is done in a standard linear regression. Each input $X_j$ is multiplied times a weight $\\beta_j$ and each input/weight product $X_j \\beta_j$ is added together. Or, in summarised form  $f(X)= \\beta_0 + \\Sigma_{j=1}^p X_j \\beta_j$. In matrix algebra form, this can be summarised as $\\theta^TX$, where $\\theta$ is a vector of weights (including $\\beta_0$), and $X$ is a vector of inputs (with an input of 0 for $\\beta_0$). The modification that logistic regression makes is to then embed the output of $\\theta^Tx$ in a new funtion $g(z)$ where $g(z)=\\frac{1}{1+e^{-z}}$. To put all this together, $h_\\theta (x) = g(\\theta^Tx)$ where $g(z)=\\frac{1}{1+e^{-z}}$. The function $g(z)$ is called the sigmoid function, and it has the beneficial property of scaling all outputs between values of 0 and 1. We can write the equations above even more succinctly by substituting in $\\theta^TX$ for $z$. Our final simplified equation is then $h_\\theta (x) = \\frac{1}{1+e{-\\theta^Tx}}$. We treat the value $h_\\theta(x)$ as our estimate of probability that x is a member of category $y=1$. The probability that $y=0$ will then be $1 - h_\\theta(x)$.  Recall that $h_\\theta(x)$ ranges from 0 to 1 thanks to our application of the sigmoid function.       \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EDA and Discussion of Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Application of Course Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
