{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/BerkeleySeal.png\" width=\"150\"/> <img src=\"resources/berkeleyischool-logo-blue.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Final project for course W261 - Machine Learning in Scale - Spring 2019</center>\n",
    "\n",
    "### __Team 13:__ Clayton Leach, James Kajdasz,  Marcelo Queiroz, Peter Trenkwalder, Vishal Agarwal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Question Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithm Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of Logistic Regression\n",
    "Our group will apply a logistic regression model to the problem. We chose logistic regression because it is a very common and widely used algorithm, and the results of a logistic regression are relatively easy to communicate and interpret. Logistic regression is a classification algorithm that uses predictor variables (continuous or categorical) to classify an outcome variable in to one of two categories. The categories are arbitrarily given labels of '0' or '1', but can apply to any decision with two possible outcomes: cancerous/non-cancerous, spam/not-spam, fraudulent/legitimate... Logistic regression can be applied to a problem with more than two categories as well by creating a separate equation for each category: A/not A, B/not B, C/not C... The outcome variable is a probability (ranging 0 to 1) of group membership. The classification with the highest probability is the predicted category label.\n",
    "\n",
    "#### Logistic Regression Equation\n",
    "Logistic regression aggregates the predictor variables similar to what is done in a standard linear regression. Each input $X_j$ is multiplied times a weight $\\beta_j$ and each input/weight product $X_j \\beta_j$ is added together. Or, in summarised form:  \n",
    "\n",
    "$\\displaystyle f(X)= \\beta_0 + \\Sigma_{j=1}^p X_j \\beta_j$ \n",
    "\n",
    "In matrix algebra form, this can be written as $\\displaystyle f(X)= \\theta^TX$, where $\\theta$ is a vector of weights (including $\\beta_0$), and $X$ is a vector of inputs (with an input of 0 for $\\beta_0$). The modification that logistic regression makes is to then embed the output of $\\theta^TX$ in a new funtion $g(z)$ where $\\displaystyle g(z)=\\frac{1}{1+e^{-z}}$. To put all this together, $h_\\theta (x) = g(\\theta^Tx)$ where $g(z)=\\frac{1}{1+e^{-z}}$. The function $g(z)$ is the sigmoid function, and it has the beneficial property of scaling all outputs between values of 0 and 1. We can write the equations above even more succinctly by substituting in $\\theta^TX$ for $z$. Our final simplified equation is then: \n",
    "\n",
    "$\\displaystyle h_\\theta (x) = \\frac{1}{1+e^{-\\theta^TX}}$ \n",
    "\n",
    "We treat the value $h_\\theta(x)$ as our estimate of probability that x is a member of category $y=1$. The probability that $y=0$ will then be $1 - h_\\theta(x)$. Both probabilities will add to one. Recall that $h_\\theta(x)$ ranges from 0 to 1 thanks to our application of the sigmoid function.       \n",
    "\n",
    "#### Cost Function\n",
    "The weights of a logistic regression equation can vary, and so there must be a way to compare one hypothetical set of weights to another to judge if one model fits the data better than another. This is done by calculating the overall error of the model when attempting to predict $y$ to summarize model fit. The function that computes the error of the model is known as the cost or loss function. The goal of a model is to fit the data in such a way as to minimize the cost function. For a given set of weights $\\theta$ attempting to preditct a label $y_i$, the cost function of $h_\\theta(X)$ can be quantified by simply squaring the errors where cost is $Cost \\displaystyle (h_\\theta (x_i), y_i) = \\frac{1}{2} (h_\\theta(X_i)-y_i)^2$. This is the standard cost function (known as squared loss) for linear regression. For logistic regression however, the squared loss function is not convex and has many local minima. Another alternative for the cost function must therefore be used. Alternatives include hinge loss, and logistic loss. The logistic loss function is the default used by Apache Spark, according to the [documentation](https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression). For logistic loss (log loss for short), when the actual value of $y$ is 1, we will take the negative log of the predicted probability of 1. When the actual value of $y$ is 0, we will take the negative log of 1 minus the predicted probability of 1. Written mathematically: $\\displaystyle Cost(h_\\theta(x),y)= \\begin{cases} -log(h_\\theta(X)) & y=1\\\\-log(1-h_\\theta(X)) & y=0\\end{cases}$ The log loss function has some nice properties. When the logistic regression correctly predicts that $\\hat{y}=1$ with a probability of 1, then $-log(1)=0$ and the loss function is zero, reflecting a perfect prediction. Similarlry, if we (correctly) predict that $\\hat{y}$ is 0 with a probability of 1, the cost function will be $-log(1-1)=0$. If however we make an incorrect prediction that $P(\\hat{y}:0)=.999$, (and the corresponding probability $P(\\hat{y}:1)=.001$) when in actuality $y=1$, then the log loss function will be $-log(.001)\\approx3$, reflecting a higher amount of error. Note that we can't take the log of 0, so instead we use values of .999 and .001. As the correct value for $\\hat{y}$ approaches a predicted probability of 0, the log loss function will approach infinity.     \n",
    "\n",
    "#### Finding the Correct Model\n",
    "One could select logistic regression weights at random, and see if the new model is an improvement over the last model by evaluating the loss function, and continue iterating. This is inefficient to do with random changes however and it's unlikley we'd ever stumble across the best model by chance. It's better to have an algorithm that systematively moves us to better and better models. There are several different algorithms to choose from, including gradient descent and stochastic gradient descent. When dealing with data at scale, consideration of algorithm must also consider the number of iterations required and how fast an algorithm will converge. For working with data at scale, the Apache Spark [documentation](https://spark.apache.org/docs/latest/mllib-optimization.html#choosing-an-optimization-method) recommends the Limited-memory Broyden-Fletcher-Golfarb-Shanno algorithm (L-BFGS for short) because it tends to converge faster in fewer iterations. The BFGS algorithm, not to be confused with the [Canadian punk rock band](https://en.wikipedia.org/wiki/Bunchofuckingoofs) is an algorithm for parameter optimization. It can be used in   \n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EDA and Discussion of Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Application of Course Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
