{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question formulation Notebook\n",
    "\n",
    "Use of toy dataset and notebook dependencies.\n",
    "\n",
    "### Notebook Set-up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "app_name = \"w261-FinalProject\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the RDDs to see the form:\n",
    "trainRDD = sc.textFile(\"gs://w261_final-project_team13/train.txt\")\n",
    "testRDD = sc.textFile(\"gs://w261_final-project_team13/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0\\t1\\t1\\t5\\t0\\t1382\\t4\\t15\\t2\\t181\\t1\\t2\\t\\t2\\t68fd1e64\\t80e26c9b\\tfb936136\\t7b4723c4\\t25c83c98\\t7e0ccccf\\tde7995b8\\t1f89b562\\ta73ee510\\ta8cd5504\\tb2cb9c98\\t37c9c164\\t2824a5f6\\t1adce6ef\\t8ba8b39a\\t891b62e7\\te5ba7672\\tf54016b9\\t21ddcdc9\\tb1252a9d\\t07b5194c\\t\\t3a171ecb\\tc5c50484\\te8b83407\\t9727dd16',\n",
       " '0\\t2\\t0\\t44\\t1\\t102\\t8\\t2\\t2\\t4\\t1\\t1\\t\\t4\\t68fd1e64\\tf0cf0024\\t6f67f7e5\\t41274cd7\\t25c83c98\\tfe6b92e5\\t922afcc0\\t0b153874\\ta73ee510\\t2b53e5fb\\t4f1b46f3\\t623049e6\\td7020589\\tb28479f6\\te6c5b5cd\\tc92f3b61\\t07c540c4\\tb04e4670\\t21ddcdc9\\t5840adea\\t60f6221e\\t\\t3a171ecb\\t43f13e8b\\te8b83407\\t731c3655']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainRDD.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\t29\\t50\\t5\\t7260\\t437\\t1\\t4\\t14\\t\\t1\\t0\\t6\\t5a9ed9b0\\ta0e12995\\ta1e14474\\t08a40877\\t25c83c98\\t\\t964d1fdd\\t5b392875\\ta73ee510\\tde89c3d2\\t59cd5ae7\\t8d98db20\\t8b216f7b\\t1adce6ef\\t78c64a1d\\t3ecdadf7\\t3486227d\\t1616f155\\t21ddcdc9\\t5840adea\\t2c277e62\\t\\t423fab69\\t54c91918\\t9b3e8820\\te75c9ae9',\n",
       " '27\\t17\\t45\\t28\\t2\\t28\\t27\\t29\\t28\\t1\\t1\\t\\t23\\t68fd1e64\\t960c983b\\t9fbfbfd5\\t38c11726\\t25c83c98\\t7e0ccccf\\tfe06fd10\\t062b5529\\ta73ee510\\tca53fc84\\t67360210\\t895d8bbb\\t4f8e2224\\tf862f261\\tb4cc2435\\t4c0041e5\\te5ba7672\\tb4abdd09\\t21ddcdc9\\t5840adea\\t36a7ab86\\t\\t32c7478e\\t85e4d73f\\t010f6491\\tee63dd9b']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testRDD.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that both are tab-separated files, so we want to sample them into a single-node computation friendly file and get back to the local machines. For that we need to know how many observations we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset count: 45840617 observations.\n",
      "Test dataset count: 6042135 observations.\n"
     ]
    }
   ],
   "source": [
    "print('Train dataset count:', trainRDD.count(), 'observations.')\n",
    "print('Test dataset count:', testRDD.count(), 'observations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on that we will take 0.3% of the dataset as sample, which will be roughly $45.840.617 \\cdot 0.0003 = 137.521$ observations and $10.38E3 \\cdot 0.0003 = 30$ MB, perfectly handle in a single node machine and still relevant. For the test dataset the same smaple ratio will be kept.\n",
    "\n",
    "Another point is that the text file do not have headers, and as we want to work with dataframes, we want to create a schema. To do that we may take a look at the `readme.txt` file supplied with the data:\n",
    "\n",
    "```\n",
    "====================================================\n",
    "\n",
    "Format:\n",
    "\n",
    "The columns are tab separeted with the following schema:\n",
    "<label> <integer feature 1> ... <integer feature 13> <categorical feature 1> ... <categorical feature 26>\n",
    "\n",
    "When a value is missing, the field is just empty.\n",
    "There is no label field in the test set.\n",
    "\n",
    "====================================================\n",
    "```\n",
    "\n",
    "Additionally we need to parse the data, going from lines of strings to integers and and strings. For that we can map the RDD after sampling and converting to the desired type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsTrain = ['label','I1','I2','I3','I4','I5','I6','I7','I8','I9','I10','I11','I12','I13',\n",
    "               'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n",
    "               'C15','C16','C17','C18','C19','C20','C21','C22','C23','C24','C25','C26']\n",
    "\n",
    "labelsTest = ['I1','I2','I3','I4','I5','I6','I7','I8','I9','I10','I11','I12','I13',\n",
    "              'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n",
    "              'C15','C16','C17','C18','C19','C20','C21','C22','C23','C24','C25','C26']\n",
    "\n",
    "toyTrainDF = trainRDD.sample(False, 0.003, 2019).map(lambda line: line.split('\\t')).toDF(labelsTrain)\n",
    "toyTestDF = testRDD.sample(False, 0.003, 2019).map(lambda line: line.split('\\t')).toDF(labelsTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy train dataframe count: 137139\n",
      "Toy test dataframe count: 18181\n"
     ]
    }
   ],
   "source": [
    "# verifying the count:\n",
    "print('Toy train dataframe count:', toyTrainDF.count())\n",
    "print('Toy test dataframe count:', toyTestDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o219.parquet.\n: java.io.IOException: No FileSystem for scheme: gs\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:452)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:548)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:278)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:547)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-3a7a36420979>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Now writing out toy datasets to be able to work on local machines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtoyTrainDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gs://w261_final-project_team13/toy_train.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtoyTestDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gs://w261_final-project_team13/toy_test.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/py4j-0.10.7-py3.6.egg/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/py4j-0.10.7-py3.6.egg/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o219.parquet.\n: java.io.IOException: No FileSystem for scheme: gs\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:452)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:548)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:278)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:547)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "# Now writing out toy datasets to be able to work on local machines\n",
    "toyTrainDF.write.parquet(\"gs://w261_final-project_team13/toy_train.txt\")\n",
    "toyTestDF.write.parquet(\"gs://w261_final-project_team13/toy_test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now running on the local machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/Assignments/FinalProject/W261-SP19-Team13-FinalProject\n"
     ]
    }
   ],
   "source": [
    "# copy the files to the local machine:\n",
    "!gsutil -m cp gs://w261_final-project_team13/toy_test.txt/* .data/toy_test.txt/\n",
    "!gsutil -m cp gs://w261_final-project_team13/toy_train.txt/* .data/toy_train.txt/\n",
    "gsutil cp gs://w261_final-project_team13/notebooks/* ./QuestionFormulation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the parquet files and print the first observations of each:\n",
    "toyTrainDF = spark.read.parquet(\"./data/toy_train.txt\")\n",
    "toyTestDF = spark.read.parquet(\"./data/toy_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(label='0', I1='1', I2='2', I3='17', I4='3', I5='685', I6='16', I7='2', I8='3', I9='7', I10='1', I11='2', I12='1', I13='3', C1='05db9164', C2='38a947a1', C3='78c2dcf9', C4='041c8b35', C5='4cf72387', C6='6f6d9be8', C7='94aa68fb', C8='1f89b562', C9='a73ee510', C10='ac25feb9', C11='577aa337', C12='5b91fbfa', C13='f405e2e8', C14='07d13a8f', C15='a8041309', C16='15913bcf', C17='3486227d', C18='998b9a30', C19='', C20='', C21='3e2fae11', C22='', C23='32c7478e', C24='09a589c1', C25='', C26='')"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toyTrainDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(I1='', I2='-1', I3='', I4='', I5='8124', I6='62', I7='5', I8='1', I9='37', I10='', I11='1', I12='', I13='', C1='05db9164', C2='08c2f5df', C3='dde182a0', C4='de1dc0c1', C5='25c83c98', C6='fbad5c96', C7='ad3508b1', C8='0b153874', C9='a73ee510', C10='965e1030', C11='ad757a5a', C12='7a27d4e1', C13='93b18cb5', C14='1adce6ef', C15='a43baafd', C16='84534f54', C17='e5ba7672', C18='29b0e3e5', C19='', C20='', C21='2b81e06c', C22='c9d4222a', C23='423fab69', C24='2f647dfe', C25='', C26='')"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toyTestDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all features are strings. We want to cast the `label` feature to _Boolean_ and the `I1` to `I13` features to _integers_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "toyTrainDF = toyTrainDF.withColumn('label', toyTrainDF.label.cast('Boolean'))\n",
    "\n",
    "intColumns = ['I1','I2','I3','I4','I5','I6','I7','I8','I9','I10','I11','I12','I13']\n",
    "for col in intColumns:\n",
    "    toyTrainDF = toyTrainDF.withColumn(col, toyTrainDF[col].cast('Integer'))\n",
    "    toyTestDF = toyTestDF.withColumn(col, toyTestDF[col].cast('Integer'))\n",
    "    \n",
    "strColumns = ['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n",
    "              'C15','C16','C17','C18','C19','C20','C21','C22','C23','C24','C25','C26']\n",
    "for column in strColumns:\n",
    "    toyTrainDF = toyTrainDF.withColumn(column, when(toyTrainDF[column] != \"\", toyTrainDF[column]).otherwise(None))\n",
    "    toyTestDF = toyTestDF.withColumn(column, when(toyTestDF[column] != \"\", toyTestDF[column]).otherwise(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(label=False, I1=1, I2=2, I3=17, I4=3, I5=685, I6=16, I7=2, I8=3, I9=7, I10=1, I11=2, I12=1, I13=3, C1='05db9164', C2='38a947a1', C3='78c2dcf9', C4='041c8b35', C5='4cf72387', C6='6f6d9be8', C7='94aa68fb', C8='1f89b562', C9='a73ee510', C10='ac25feb9', C11='577aa337', C12='5b91fbfa', C13='f405e2e8', C14='07d13a8f', C15='a8041309', C16='15913bcf', C17='3486227d', C18='998b9a30', C19=None, C20=None, C21='3e2fae11', C22=None, C23='32c7478e', C24='09a589c1', C25=None, C26=None)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toyTrainDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(I1=None, I2=-1, I3=None, I4=None, I5=8124, I6=62, I7=5, I8=1, I9=37, I10=None, I11=1, I12=None, I13=None, C1='05db9164', C2='08c2f5df', C3='dde182a0', C4='de1dc0c1', C5='25c83c98', C6='fbad5c96', C7='ad3508b1', C8='0b153874', C9='a73ee510', C10='965e1030', C11='ad757a5a', C12='7a27d4e1', C13='93b18cb5', C14='1adce6ef', C15='a43baafd', C16='84534f54', C17='e5ba7672', C18='29b0e3e5', C19=None, C20=None, C21='2b81e06c', C22='c9d4222a', C23='423fab69', C24='2f647dfe', C25=None, C26=None)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toyTestDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can start analyzing the data to understand which features are more likely to contribute to a model and which are more likely to bias our model (due to a high number of `None` values or that need normalization, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
